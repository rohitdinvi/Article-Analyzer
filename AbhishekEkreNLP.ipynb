{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e562be9",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-family: 'Times New Roman'; font-size: 24px;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adae7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Download the CMU Pronouncing Dictionary for syllable counting\n",
    "\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7362235",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Times New Roman'; font-size: 24px;\">Creating the Dataframe and Webscraping </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c08845",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile(r'C:\\Users\\ACER\\Downloads\\Input (1).xlsx', engine='openpyxl')\n",
    "df=pd.read_excel(xlsx)\n",
    "\n",
    "article=[]\n",
    "for i in df.URL:\n",
    "    text_content=''\n",
    "    webpage=requests.get(i)\n",
    "    soup=BeautifulSoup(webpage.content,'lxml')\n",
    "    for j in soup.find_all('div',class_=[\"td-post-content tagdiv-type\",\"tdb-block-inner td-fix-index\"]):\n",
    "        text_content += j.get_text() + '\\n'\n",
    "    article.append(text_content)  \n",
    "\n",
    "df.insert(2,'article',article,True)\n",
    "\n",
    "#cleaning the webscraping content\n",
    "for i in range(0, 100):\n",
    "    index = df.iloc[i]['article'].find('Search')\n",
    "    x = str(df.iloc[i]['article'])\n",
    "    df.iloc[i, df.columns.get_loc('article')] = x[index + 6:]\n",
    "\n",
    "#saving the articles in a text file:\n",
    "article_title=[]\n",
    "for i in df.URL:\n",
    "    text_content=''\n",
    "    webpage=requests.get(i)\n",
    "    soup=BeautifulSoup(webpage.content,'lxml')\n",
    "    for j in soup.find_all('h1',class_=[\"entry-title\"]):\n",
    "        text_content += j.get_text() \n",
    "    article_title.append(text_content)\n",
    "df['article_title']=article_title\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4da6e",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Times New Roman'; font-size: 24px;\">Text preprocessing</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e1730",
   "metadata": {},
   "source": [
    "Developing functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571d6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing\n",
    "df['article']=df['article'].str.lower()\n",
    "\n",
    "#removing urls\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub('',text)\n",
    "\n",
    "# removing \\n's \n",
    "def removingns(text):\n",
    "    return text.replace('\\n',' ')\n",
    "\n",
    "#removing html \n",
    "def remove_html(text):\n",
    "    html_tags_pattern = re.compile(r'<.*?>')\n",
    "    return html_tags_pattern.sub(r'',text)\n",
    "\n",
    "#removing punctuation\n",
    "def remove_punc(text):\n",
    "    punctuations=string.punctuation\n",
    "    for i in punctuations:\n",
    "        text=text.replace(i,'')\n",
    "    return text\n",
    "\n",
    "\n",
    "# extracting of positve and negative stopwords\n",
    "path=r'C:\\Users\\ACER\\Downloads\\StopWords-20240302T184700Z-001\\StopWords'\n",
    "s=[]\n",
    "stopwords=[]\n",
    "for i in os.listdir(path):\n",
    "    file_path=os.path.join(path,i)\n",
    "    file=open(file_path)\n",
    "    for i in file:\n",
    "        s.append(i.strip('\\n').lower())\n",
    "for i in s:\n",
    "    for k in i.split('|'):\n",
    "        stopwords.append(k)\n",
    "\n",
    "# creating master dictionary of positive words and negative words\n",
    "path=r'C:\\Users\\ACER\\Downloads\\MasterDictionary-20240302T110426Z-001\\MasterDictionary'\n",
    "flag=0 #flag for checking postivefile or negativefile\n",
    "positive_word_list=[]\n",
    "negative_word_list=[]\n",
    "for i in os.listdir(path):\n",
    "    file_path=os.path.join(path,i)\n",
    "    file=open(file_path)\n",
    "    for i in file:\n",
    "        if flag==0:\n",
    "            if i not in stopwords:\n",
    "                negative_word_list.append(i.strip('\\n'))\n",
    "        else:\n",
    "            if i not in stopwords:\n",
    "                positive_word_list.append(i.strip('\\n'))\n",
    "    flag=flag+1\n",
    "    \n",
    "#creating dictionary out of it\n",
    "positive_word={}\n",
    "negative_word={}\n",
    "for word in positive_word_list:\n",
    "    positive_word[word] = True\n",
    "for word in negative_word_list:\n",
    "    negative_word[word]=True\n",
    "\n",
    "\n",
    "def remove_stopword(text):\n",
    "    new_text=''\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            new_text+=word+' '\n",
    "    return new_text\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemmatization(tokens):\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c03f6a",
   "metadata": {},
   "source": [
    "Calling the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0b913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aritcle']=df['article'].apply(remove_url)\n",
    "df['article']=df['article'].apply(removingns)\n",
    "df['article']=df['article'].apply(remove_html)\n",
    "\n",
    "#preserving the article before removing stopwords from it \n",
    "df['b4Cleanarticle']=df['article']\n",
    "\n",
    "\n",
    "df['article']=df['article'].apply(remove_stopword)\n",
    "\n",
    "#using built-in function for tokenization\n",
    "df['token']=df['article'].apply(word_tokenize)\n",
    "\n",
    "df['lemmatized_tokens']=df['token'].apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f281a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "95    None\n",
       "96    None\n",
       "97    None\n",
       "98    None\n",
       "99    None\n",
       "Length: 100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the path according to your needs\n",
    "path=r'E:\\text_articles'\n",
    "i=0\n",
    "def filetextmaker(row):\n",
    "    global i\n",
    "    filename=str(row['URL_ID'])\n",
    "    file_path=os.path.join(path,filename)\n",
    "    f=open(file_path+'.txt','w',encoding='utf-8')\n",
    "    f.write(row['article_title'] +'\\n')\n",
    "    f.write(row['b4Cleanarticle'])\n",
    "    print(i)\n",
    "    i+=1\n",
    "df.apply(filetextmaker,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce6392",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Times New Roman'; font-size: 24px;\">Making Functions for Output variables (postive,negative,polarity and subjectivity score)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454d2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positivity score\n",
    "def positivity(lis):\n",
    "    positive=0\n",
    "    for i in lis:\n",
    "        if i in positive_word_list:\n",
    "            positive=positive+1\n",
    "    return positive\n",
    "\n",
    "#negaitivity score\n",
    "def negativity(lis):\n",
    "    negative=0\n",
    "    for i in lis:\n",
    "        if i in negative_word_list:\n",
    "            negative=negative-1\n",
    "    return negative*-1\n",
    "\n",
    "#subjectivity score\n",
    "def subjectivity(lis,posit,negat):\n",
    "    words_afterclean=len(lis)\n",
    "    subjectivity_score=(posit+negat)/((words_afterclean)+0.000001)\n",
    "    return subjectivity_score\n",
    "#polarity score\n",
    "def polarity(posit,negat):\n",
    "    polarity_score=(posit-negat)/((posit+negat)+0.000001)\n",
    "    return polarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9a2d1",
   "metadata": {},
   "source": [
    "Calling the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35548f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive_score']=df['lemmatized_tokens'].apply(positivity)\n",
    "df['negative_score']=df['lemmatized_tokens'].apply(negativity)\n",
    "df['subjectivity_score']=df.apply(lambda row: subjectivity(row['lemmatized_tokens'],row['positive_score'],row['negative_score']),axis=1 )\n",
    "df['polarity_score'] = df.apply(lambda row: polarity(row['positive_score'], row['negative_score']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8415dd2",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a08ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_token']=df['b4Cleanarticle'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757cd08d",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Times New Roman'; font-size: 24px;\">Making Functions \n",
    " for Output variables \n",
    "<p style=\"font-family: 'Times New Roman'; font-size: 20px;\">\n",
    "avg sentence length<br>\n",
    "percentage of complex words<br>\n",
    "fog index<br>\n",
    "avg number of words per sentence<br>\n",
    "complex word count<br>\n",
    "word count<br>\n",
    "syllable per word<br>\n",
    "personal pronouns<br>\n",
    "avg word length\n",
    "    \n",
    "</p> </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2495ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n",
    "def syllable_count(word):\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "    else:\n",
    "        return [1]\n",
    "def is_complex(word, threshold=3):\n",
    "    syllables = syllable_count(word)\n",
    "    return any(s >= threshold for s in syllables)\n",
    "\n",
    "def sentence_count(lis):\n",
    "    sent_count=0\n",
    "    word_key=0\n",
    "    l=[]\n",
    "    for i in lis:\n",
    "        global article_Count\n",
    "        word_key=word_key+1\n",
    "        word_value=[]\n",
    "        word_count=0\n",
    "        for j in i.split():\n",
    "            word_count+=1\n",
    "        word_value.append(word_count)\n",
    "        sent_count=sent_count+1\n",
    "        word.append(word_count)\n",
    "        l=word\n",
    "    article_Count+=1\n",
    "    dict[article_Count]=l\n",
    "    \n",
    "    if(sent_count!=0):\n",
    "        sent.append(sent_count)\n",
    "    else:\n",
    "        sent.append(sent_count)\n",
    "        sent_count=1\n",
    "        \n",
    "#finding error indexes where division by zero is ocuuring count=0 \n",
    "\n",
    "def readability(text):\n",
    "    \n",
    "    global sent_index,avg_sentencelength,complexpercent,complex_word,fog_index,avg_sentlength\n",
    "    if(sent_index<100):\n",
    "        #no. of words in whole text\n",
    "        word_count=len(text.split())\n",
    "    #no. of complex words in text\n",
    "        complex_word = sum(is_complex(word) for word in text.split())\n",
    "        complex_wordCount.append(complex_word)\n",
    "    \n",
    "        avg_sentlength=word_count/sent[sent_index]\n",
    "      \n",
    "        avg_sentencelength.append(avg_sentlength)\n",
    "    \n",
    "        if(word_count!=0):\n",
    "            complexpercent=complex_word/word_count\n",
    "            complex_percent.append(complexpercent)\n",
    "        else:\n",
    "            complex_percent.append(complexpercent)\n",
    "            complexpercent=0\n",
    " \n",
    "    fogindex=0.4*(avg_sentlength+complexpercent)\n",
    "    fog_index.append(fogindex)\n",
    "    sent_index+=1\n",
    "   \n",
    "    return avg_sentlength,complexpercent,complex_word\n",
    "\n",
    "syllable_error_index=0\n",
    "\n",
    "def syllableperword(text):\n",
    "    global index,syllable_error_index\n",
    "    #handling exception using regex \n",
    "    pattern = r'\\b(\\w+)(?:es|ed)\\b'\n",
    "    clean_text = re.sub(pattern, r'\\1', text)\n",
    "    syllablesum=0\n",
    "    word_cunt=0\n",
    "    for i in clean_text.split():\n",
    "        val=syllable_count(i)\n",
    "        if val:\n",
    "            syllablesum += val[0]  \n",
    "        else:\n",
    "            missing_syllable.append(syllable_error_index)\n",
    "        word_cunt += 1\n",
    "\n",
    "    if word_cunt != 0:\n",
    "        return syllablesum / word_cunt\n",
    "    else:\n",
    "        return 'NaN'  \n",
    "    syllable_error_index += 1\n",
    "    \n",
    "def count_pronouns(text):\n",
    "    #not made exception for US as it is removed by stopword\n",
    "    pronoun_pattern = r'\\b(i|we|my|ours|us)\\b'\n",
    "    pronouns_found = re.findall(pronoun_pattern, text, flags=re.IGNORECASE)\n",
    "    pronoun_count = len(pronouns_found)\n",
    "    \n",
    "    return pronoun_count\n",
    "\n",
    "def nltstopwordremover(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "def word_count(text):\n",
    "    unpunctuated=remove_punc(text) \n",
    "    readytext=nltstopwordremover(text)\n",
    "    wordCount=len(readytext.split())\n",
    "    return wordCount\n",
    "\n",
    "def avgwordlen(text):\n",
    "    words = text.split()\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    if total_words > 0:\n",
    "        average_word_length = total_characters / total_words\n",
    "    else:\n",
    "        average_word_length = 0  \n",
    "    return average_word_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5fad6",
   "metadata": {},
   "source": [
    "Calling the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afb7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where replacements occurred: [35, 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent=[]\n",
    "word=[]\n",
    "dict={}\n",
    "article_Count =0\n",
    "complex_wordCount=[]\n",
    "sent_index=0\n",
    "avg_sentencelength=[]\n",
    "fog_index=[]\n",
    "complex_percent=[]\n",
    "word_cou=[]\n",
    "missing_syllable=[]\n",
    "df['sentence_token'].apply(sentence_count)\n",
    "\n",
    "#finding discrepancies and handling them\n",
    "ind = []\n",
    "for idx, value in enumerate(sent):\n",
    "    if value == 0:\n",
    "        ind.append(idx)\n",
    "        sent[idx] = 1\n",
    "print(\"Indices where replacements occurred:\", ind)\n",
    "\n",
    "df['b4Cleanarticle'].apply(readability)\n",
    "\n",
    "df['ComplexPercent']=complex_percent\n",
    "df['AvgSentenceLength']=avg_sentencelength\n",
    "df['Avgwordspersent']=avg_sentencelength\n",
    "df['Complexwordcount']=complex_wordCount\n",
    "df['FogIndex']=fog_index\n",
    "\n",
    "df['syllableperword']=df['b4Cleanarticle'].apply(syllableperword)\n",
    "df['pronoun_count']=df['b4Cleanarticle'].apply(count_pronouns)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "df['wordCount']=df['b4Cleanarticle'].apply(word_count)\n",
    "df['AverageWordLength']=df['b4Cleanarticle'].apply(avgwordlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae33c8a",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Times New Roman'; font-size: 24px;\">Creating the final Dataframe and Exporting it as Excel</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62c0df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             URL_ID                                                URL  \\\n",
      "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
      "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
      "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
      "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
      "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
      "..              ...                                                ...   \n",
      "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
      "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
      "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
      "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
      "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
      "\n",
      "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
      "0               27               6        0.636364            0.050152   \n",
      "1               54              31        0.270588            0.089005   \n",
      "2               39              25        0.218750            0.083009   \n",
      "3               38              80       -0.355932            0.154653   \n",
      "4               18               8        0.384615            0.061033   \n",
      "..             ...             ...             ...                 ...   \n",
      "95              28              61       -0.370787            0.137134   \n",
      "96              25              36       -0.180328            0.107774   \n",
      "97               5               3        0.250000            0.030888   \n",
      "98              17               3        0.700000            0.049383   \n",
      "99              36              62       -0.265306            0.137447   \n",
      "\n",
      "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
      "0             19.078125                     0.094185   7.668924   \n",
      "1             18.262500                     0.177276   7.375910   \n",
      "2             18.701754                     0.230769   7.573009   \n",
      "3             22.425532                     0.202087   9.051048   \n",
      "4             17.461538                     0.164464   7.050401   \n",
      "..                  ...                          ...        ...   \n",
      "95            23.291667                     0.181574   9.389296   \n",
      "96            28.342105                     0.110492  11.381039   \n",
      "97            32.000000                     0.203125  12.881250   \n",
      "98            19.909091                     0.103501   8.005037   \n",
      "99            34.125000                     0.146520  13.708608   \n",
      "\n",
      "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
      "0                          19.078125                 115          27   \n",
      "1                          18.262500                 259          54   \n",
      "2                          18.701754                 246          39   \n",
      "3                          22.425532                 213          38   \n",
      "4                          17.461538                 112          18   \n",
      "..                               ...                 ...         ...   \n",
      "95                         23.291667                 203          28   \n",
      "96                         28.342105                 119          25   \n",
      "97                         32.000000                  78           5   \n",
      "98                         19.909091                  68          17   \n",
      "99                         34.125000                 160          36   \n",
      "\n",
      "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
      "0            1.37674                 12        19.078125  \n",
      "1           1.574949                  6        18.262500  \n",
      "2           1.678236                 13        18.701754  \n",
      "3           1.632827                  5        22.425532  \n",
      "4           1.506608                  6        17.461538  \n",
      "..               ...                ...              ...  \n",
      "95          1.543828                  4        23.291667  \n",
      "96           1.35376                  7        28.342105  \n",
      "97          1.572917                  0        32.000000  \n",
      "98          1.375951                  4        19.909091  \n",
      "99          1.462454                  3        34.125000  \n",
      "\n",
      "[100 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "finaldict={\n",
    "    'URL_ID':df['URL_ID'],\n",
    "    'URL':df['URL'],\n",
    "    'POSITIVE SCORE':df['positive_score'],\n",
    "    'NEGATIVE SCORE':df['negative_score'],\n",
    "    'POLARITY SCORE':df['polarity_score'],\n",
    "    'SUBJECTIVITY SCORE':df['subjectivity_score'],\n",
    "    'AVG SENTENCE LENGTH':df['AvgSentenceLength'],\n",
    "    'PERCENTAGE OF COMPLEX WORDS':df['ComplexPercent'],\n",
    "    'FOG INDEX':df['FogIndex'],\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE':df['AvgSentenceLength'],\n",
    "    'COMPLEX WORD COUNT':df['Complexwordcount'],\n",
    "    'WORD COUNT':df['positive_score'],\n",
    "    'SYLLABLE PER WORD':df['syllableperword'],\n",
    "    'PERSONAL PRONOUNS':df['pronoun_count'],\n",
    "    'AVG WORD LENGTH':df['Avgwordspersent']\n",
    "}\n",
    "finaldf=pd.DataFrame(finaldict)\n",
    "print(finaldf)\n",
    "file_name = 'Output Data Structure.xlsx'\n",
    "finaldf.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a30b47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the version of Pandas\n",
    "print(\"Pandas version:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6cc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
